!["Problem"](./images/lecture13/img1.JPG)

!["Problem"](./images/lecture13/img2.JPG)

!["Problem"](./images/lecture13/img3.JPG)

!["Problem"](./images/lecture13/img4.JPG)

!["Problem"](./images/lecture13/img5.JPG) 

!["Problem"](./images/lecture13/img6.JPG) 

!["Problem"](./images/lecture13/img7.JPG) 

!["Problem"](./images/lecture13/img8.JPG) 

!["Problem"](./images/lecture13/img9.JPG) 

!["Problem"](./images/lecture13/img10.JPG)

!["Problem"](./images/lecture13/img11.JPG)

!["Problem"](./images/lecture13/img12.JPG)

!["Problem"](./images/lecture13/img13.JPG)

## PixleRNN and PixleCNN

!["Problem"](./images/lecture13/img14.JPG)

so in this case what they do is we have this image data X that we have and we want to model the probability or likelihood of this image P of X. Right and so in this case, for these kinds of models, we use the chain rule to decompose this likelihood into a product of one dimensional distribution. So we have here the probability of each pixel X I conditioned on all previous pixels X1 through XI - 1. and your likelihood all right, your joint likelihood of all the pixels in your image is going to be the product of all of these pixels together, all of these likelihoods together. And then once we define this likelihood, in order to train this model we can just maximize the likelihood of our training data under this defined density. 

So if we look at this this distribution over pixel values right, we have this P of XI given all the previous pixel values, well this is a really complex distribution. So how can we model this? Well we've seen before that if we want to have complex transformations we can do these using neural networks. Neural networks are a good way to express complex transformations. And so what we'll do is we'll use a neural network to express this complex function that we have of the distribution. And one thing you'll see here is that, okay even if we're going to use a neural network for this another thing we have to take care of is how do we order the pixels. Right, I said here that we have a distribution for P of XI given all previous pixels 

!["Problem"](./images/lecture13/img15.JPG)

So PixelRNN was a model proposed in 2016 that basically defines a way for setting up and optimizing this problem and so how this model works is that we're going to generate pixels starting in a corner of the image. So we can look at this grid as basically the pixels of your image and so what we're going to do is start from the pixel in the upper left-hand corner and then we're going to sequentially generate pixels based on these connections from the arrows that you can see here. And each of the dependencies on the previous pixels in this ordering is going to be modeled using an RNN or more specifically an LSTM which we've seen before in lecture. Right so using this we can basically continue to move forward just moving down a long is diagonal and generating all of these pixel values dependent on the pixels that they're connected to. And so this works really well but the drawback here is that this sequential generation, right, so it's actually quite slow to do this. You can imagine you know if you're going to generate a new image instead of all of these feed forward networks that we see, we've seen with CNNs. Here we're going to have to iteratively go through and generate all these images, all these pixels. 

## PixelCNN

!["Problem"](./images/lecture13/img16.JPG)

And this has very similar setup as pixelCNN and we're still going to do this image generation starting from the corner of the of the image and expanding outwards but the difference now is that now instead of using  an RNN to model all these dependencies we're going to use the CNN instead. And we're now going to use a CNN over a a context region that you can see here around in the particular pixel that we're going to generate now. Right so we take the pixels around it, this gray area within the region that's already been generated and then we can pass this through a CNN and use that to generate our next pixel value. And so what this is going to give is this is going to give This is a CNN, a neural network at each pixel location right and so the output of this is going to be a soft max loss over the pixel values here. In this case we have a 0 to 255 and then we can train this by maximizing the likelihood of the training images. Right so we say that basically we want to take a training image we're going to do this generation process and at each pixel location we have the ground truth training data image value that we have here and this is a quick basically the label or the the the classification label that we want our pixel to be which of these 255 values and we can train this using a Softmax loss. Right and so basically the effect of doing this is that we're going to maximize the likelihood of our training data pixels being generated. 

- Yeah, so the question is, I thought we were talking about unsupervised learning, why do we have basically a classification label here? The reason is that this loss, this output that we have is the value of the input training data. So we have no external labels, right? We didn't go and have to manually collect any labels for this, we're just taking our input data and saying that this is what we used for the last function. 

!["Problem"](./images/lecture13/img17.JPG)

So using pixelCNN training is faster than pixelRNN because here now right at every pixel location we want to maximize the value of our, we want to maximize the likelihood of our training data showing up and so we have all of these values already right, just from our training data and so we can do this much faster but a generation time for a test time we want to generate a completely new image right, just starting from the corner and we're not, we're not trying to do any type of learning so in that generation time we still have to generate each of these pixel locations before we can generate the next location. And so generation time here it still slow even though training time is faster. 

So again, how do you pick this distribution? So at training time you have these distributions from your training data and then at generation time you can just initialize this with either uniform or from your training data, however you want. And then once you have that everything else is conditioned based on that. 

Pixle distribution depends on training data.

what the chain rule allows us to do is it allows us to find this very tractable density that we can then basically optimize and do, directly optimizes likelihood 

!["Problem"](./images/lecture13/img18.JPG)

!["Problem"](./images/lecture13/img19.JPG)

!["Problem"](./images/lecture13/img20.JPG)

## Variational Autoencoders(VAE)

!["Problem"](./images/lecture13/img21.JPG)

All right so in this case we have our input data X and then we're going to want to learn some features that we call Z. And then we'll have an encoder that's going to be a mapping, a function mapping from this input data to our feature Z. And this encoder can take many different forms right, they would generally use neural networks so originally these models have been around, autoencoders have been around for a long time. So in the 2000s we used linear layers of non-linearities, then later on we had fully connected deeper networks and then after that we moved on to using CNNs for these encoders. So we take our input data X and then we map this to some feature Z. And Z we usually have as, we usually specify this to be smaller than X and we perform basically dimensionality reduction because of that. 

So the question who has an idea of why do we want to do dimensionality reduction here? Why do we want Z to be smaller than X? 
Z should represent the most important features in X. So we want Z to be able to learn features that can capture meaningful factors of variation in the data.

!["Problem"](./images/lecture13/img22.JPG)

So what we want is we want to have input data that we use an encoder to map it to some lower dimensional features Z. This is the output of the encoder network, and we want to be able to take these features that were produced based on this input data and then use a decoder a second network and be able to output now something of the same size dimensionality as X and have it be similar to X right so we want to be able to reconstruct the original data. And again for the decoder we are basically using same types of networks as encoders so it's usually a little bit symmetric and now we can use CNN networks for most of these. 

!["Problem"](./images/lecture13/img23.JPG)

Right in the reason why we have a convolutional network for the encoder and an upconvolutional network for the decoder is because at the encoder we're basically taking it from this high dimensional input to these lower dimensional features and now we want to go the other way go from our low dimensional features back out to our high dimensional reconstructed input. 

!["Problem"](./images/lecture13/img24.JPG)

And so in order to get this effect that we said we wanted before of being able to reconstruct our input data we'll use something like an L2 loss function. Right that basically just says let me make my pixels of my input data to be the same as my, my pixels in my reconstructed data to be the same as the pixels of my input data. An important thing to notice here, this relates back to a question that we had earlier, is that even though we have this loss function here, there's no, there's no external labels that are being used in training this. All we have is our training data that we're going to use both to pass through the network as well as to compute our loss function. 

!["Problem"](./images/lecture13/img25.JPG)

!["Problem"](./images/lecture13/img26.JPG)

Right and so for example we can now go from this input to our features and then have an additional classifier network on top of this that now we can use to output a class label for example for classification problem we can have external labels from here and use our standard loss functions like Softmax. And so the value of this is that we basically were able to use a lot of unlabeled training data to try and learn good general feature representations. Right, and now we can use this to initialize a supervised learning problem where sometimes we don't have so much data we only have small data. 

