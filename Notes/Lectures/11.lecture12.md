!["Problem"](./images/lecture12/img1.JPG)

!["Problem"](./images/lecture12/img2.JPG)

We can get some since for what these filters are looking for by simply visualizing the learned weights of these filters as images themselves. So, for each of those 11 by 11 by 3 filters in AlexNet, we can just visualize that filter as a little 11 by 11 image with a three channels give you the red, green and blue values. And then because there are 64 of these filters we just visualize 64 little 11 by 11 images. And we can repeat... So we have shown here at the. 

Why does visualizing the weights of the filters? Tell you what the filter is looking for. So this intuition comes from sort of template matching and inner products. That if you imagine you have some, some template vector. And then you imagine you compute a scaler output by taking inner product between your template vector and some arbitrary piece of data. Then, the input which maximizes that activation. Under a norm constraint on the input is exactly when those two vectors match up. So, in that since that, when, whenever you're taking inner products, the thing causes an inner product to excite maximally is a copy of the thing you are taking an inner product with. So, that, that's why we can actually visualize these weights and that, why that shows us, what this first layer is looking for. So, for these networks the first layers always was a convolutional layer. So, generally whenever you are looking at image. Whenever you are thinking about image data and training convolutional networks, you generally put a convolutional layer at the first, at the first stop. 

!["Problem"](./images/lecture12/img3.JPG)

Though we are printing but as the weights in the intermediate layers does not have any connection to the input layer it kinda doesn't make any sense to us , so we need to develop some fancy technique to visualize.

!["Problem"](./images/lecture12/img4.JPG)

Now at the last layer remember when we looking at the last layer of convolutional network. We have these maybe 1000 class scores that are telling us what are the predicted scores for each of the classes in our training data set and immediately before the last layer we often have some fully connected layer. In the case of Alex net we have some 4096- dimensional features representation of our image that then gets fed into that final our final layer to predict our final class scores. And one another, another kind of route for tackling the problem of visual, visualizing and understanding ConvNets is to try to understand what's happening at the last layer of a convolutional network. So what we can do is how to take some, some data set of images run a bunch of, run a bunch of images through our trained convolutional network and recorded that 4096 dimensional vector for each of those images. And now go through and try to figure out and visualize that last layer, that last hidden layer rather than those rather than the first convolutional layer. 

!["Problem"](./images/lecture12/img5.JPG)

So, one thing you might imagine is, is trying a nearest neighbor approach. So, remember, way back in the second lecture we saw this graphic on the left where we, where we had a nearest neighbor classifier. Where we were looking at nearest neighbors in pixels space between CIFAR 10 images. And then when you look at nearest neighbors in pixel space between CIFAR 10 images you see that you pull up images that looks quite similar to the query image. So again on the left column here is some CIFAR 10 image from the CIFAR 10 data set and then these, these next five columns are showing the nearest neighbors in pixel space to those test set images. And so for example this white dog that you see here, it's nearest neighbors are in pixel space are these kinds of white blobby things that may, may or may not be dogs, but at least the raw pixels of the image are quite similar. So now we can do the same type of visualization computing and visualizing these nearest neighbor images. But rather than computing the nearest neighbors in pixel space, instead we can compute nearest neighbors in that 4096 dimensional feature space. Which is computed by the convolutional network. So here on the right we see some examples. So this, this first column shows us some examples of images from the test set of image that... Of the image net classification data set and now the, these subsequent columns show us nearest neighbors to those test set images in the 4096, in the 4096th dimensional features space computed by Alex net. And you can see here that this is quite different from the pixel space nearest neighbors, because the pixels are often quite different. between the image in it's nearest neighbors and feature space. However, the semantic content of those images tends to be similar in this feature space. So for example, if you look at this second layer the query image is this elephant standing on the left side of the image with a screen grass behind him. and now one of these, one of these... it's third nearest neighbor in the tough set is actually an elephant standing on the right side of the image. So this is really interesting. Because between this elephant standing on the left and this element stand, elephant standing on the right the pixels between those two images are almost entirely different. However, in the feature space which is learned by the network those two images and that being very close to each other. Which means that somehow this, this last their features is capturing some of those semantic content of these images. That's really cool and really exciting and, and in general looking at these kind of nearest neighbor visualizations is really quick and easy way to visualize something about what's going on here.

The question is, what is the nearest... What is this nearest neighbor thing have to do at the last layer? So we're taking this image we're running it through the network and then the, the second to last like the last hidden layer of the network is of 4096th dimensional vector. Because there's this, this is... This is there, there are these fully connected layers at the end of the network. So we are doing is... We're writing down that 4096th dimensional vector for each of the images and then we are computing nearest neighbors according to that 4096th dimensional vector. Which is computed by, computed by the network. 

!["Problem"](./images/lecture12/img6.JPG)

Standing for t-distributed stochastic neighbor embeddings. Which is slightly more powerful method. Which is a non-linear dimensionality reduction method that people in deep often use for visualizing features. 

t-sne visualization on the right of them image 

This visualization here is, is showing a t-SNE dimensionality reduction on the emnest data set. So, emnest remember is this date set of hand written digits between zero and nine. Each image is a gray scale image 20... 28 by 28 gray scale image and now we're... So that Now we've, we've used t-SNE to take that 28 times 28 dimensional features space of the raw pixels for m-nest and now compress it down to two- dimensions ans then visualize each of those m-nest digits in this compress two-dimensional representation and when you do, when you run t-SNE on the raw pixels and m-nest 
You can see these natural clusters appearing. Which corresponds to the, the digits of these m-nest of, of these m-nest data set. 


!["Problem"](./images/lecture12/img7.JPG)

we, we have an image so now we end up with three different pieces of information about each image. We have the pixels of the image. We have the 4096th dimensional vector. Then we use t-SNE to convert the 4096th dimensional vector into a two-dimensional coordinate and then we take the original pixels of the image and place that at the two-dimensional coordinate corresponding to the dimensionality reduced version of the 4096th dimensional feature. 

Visualizing activation will help

!["Problem"](./images/lecture12/img8.JPG)

!["Problem"](./images/lecture12/img9.JPG)

So another, another kind of useful thing we can do for visualizing intermediate features is... Visualizing what types of patches from input images cause maximal activation in different, different features, different neurons. So what we've done here is that, we pick... Maybe again the con five layer from Alex Net? And remember each of these activation volumes at the con, at the con five in Alex net gives us a 128 by 13 by 13 chunk of numbers. Then we'll pick one of those 128 channels. Maybe channel 17 and now what we'll do is run many images through this convolutional network. And then, for each of those images record the con five features and then look at the... Right, so, then, then look at the, the... The parts of that 17th feature map that are maximally activated over our data set of images. And now, because again this is a convolutional layer each of those neurons in the convolutional layer has some small receptive field in the input. Each of those neurons is not looking at the whole image. They're only looking at the sub set of the image. Then what we'll do is, is visualize the patches from the, from this large data set of images corresponding to the maximal activations of that, of that feature, of that particular feature in that particular layer. And then we can sorts these out, sort these patches by their activation at that, at that particular layer.

And now at the bottom we also see some maximally activating patches for neurons from a higher up layer in the same network. And now because they are coming from higher in the network they have a larger receptive field. So, they're looking at larger patches of the input image and we can also see that they're looking for maybe larger structures in the input image. So this, this second row is maybe looking, it seems to be looking for human, humans or maybe human faces. We have maybe something looking for... Parts of cameras or different types of larger, larger, larger object like type things, types of things.

!["Problem"](./images/lecture12/img10.JPG) 

is this idea of an exclusion experiment. So, what we want to do is figure out which parts of the input, of the input image cause the network to make it's classification decision. So, what we'll do is, we'll take our input image in this case an elephant and then we'll block out some part of that, some region in that input image and just replace it with the mean pixel value from the data set. And now, run that occluded image throughout, through the network and then record what is the predicted probability of this occluded image? And now slide this occluded patch over every position in the input image and then repeat the same process. And then draw this heat map showing, what was the predicted probability output from the network as a function of where did, which part of the input image did we occlude? And the idea is that if when we block out some part of the image if that causes the network score to change drastically. Then probably that part of the input image was really important for the classification decision. 

So, maybe this example of a Go-kart at the bottom, you can see over here that when we, so here, red, the, the red corresponds to a low probability and the white and yellow corresponds to a high probability. So when we block out the region of the image corresponding to this Go-kart in front. Then the predicted probability for the Go-kart class drops a lot. So that gives us some sense that the network is actually caring a lot about these, these pixels in the input image in order to make it's classification decision. 

!["Problem"](./images/lecture12/img11.JPG)

!["Problem"](./images/lecture12/img12.JPG)

!["Problem"](./images/lecture12/img13.JPG)

So again, we have the same question of given an input image of a dog in this case and the predicted class label of dog we want to know which pixels in the input image are important for classification. 

And this will directly tell us in this sort of, first order approximation sense. For each input, for each pixel in the input image if we wiggle that pixel a little bit then how much will the classification score for the class change? And this is another way to get at this question of which pixels in the input matter for the classification. And when we, and when we run for example Saliency, where computer Saliency map for this dog, we see kind of a nice outline of a dog in the image. Which tells us that these are probably the pixels of that, network is actually looking at, for this image.

!["Problem"](./images/lecture12/img14.JPG)

!["Problem"](./images/lecture12/img15.JPG)