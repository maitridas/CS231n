!["Problem"](./images/lecture14/img1.JPG)

!["Problem"](./images/lecture14/img2.JPG)

!["Problem"](./images/lecture14/img3.JPG)

!["Problem"](./images/lecture14/img4.JPG)

!["Problem"](./images/lecture14/img5.JPG)

!["Problem"](./images/lecture14/img6.JPG)

!["Problem"](./images/lecture14/img7.JPG)

!["Problem"](./images/lecture14/img8.JPG)

!["Problem"](./images/lecture14/img9.JPG)

!["Problem"](./images/lecture14/img10.JPG)

!["Problem"](./images/lecture14/img11.JPG)

so if you look at a random policy here, a random policy would consist of, basically, at any given state or cell that you're in just sampling randomly which direction that you're going to move in next. Right, so all of these have equal probability. On the other hand, an optimal policy that we would like to have is basically taking the action, the direction that will move us closest to a terminal state. So you can see here, if we're right next to one of the terminal states we should always move in the direction that gets us to this terminal state. And otherwise, if you're in one of these other states, you want to take the direction that will take you closest to one of these states. 

!["Problem"](./images/lecture14/img12.JPG)

!["Problem"](./images/lecture14/img13.JPG)

!["Problem"](./images/lecture14/img14.JPG)

So let's consider this a Q-value function from the optimal policy Q star, which is then going to satisfy this Bellman equation, which is this identity shown here, and what this means is that given any state, action pair, s and a, the value of this pair is going to be the reward that you're going to get, r, plus the value of whatever state that you end up in. So, let's say, s prime. And since we know that we have the optimal policy, then we also know that we're going to play the best action that we can, right, at our state s prime. And so then, the value at state s prime is just going to be the maximum over our actions, a prime, of Q star at s prime, a prime. And so then we get this identity here, for optimal Q-value. Right, and then also, as always, we have this expectation here, because we have randomness over what state that we're going to end up in. And then we can also infer, from here, that our optimal policy, right, is going to consist of taking the best action in any state, as specified by Q star. Q star is going to tell us of the maximum future reward that we can get from any of our actions, so we should just take a policy that's following this and just taking the action that's going to lead to best reward. 

!["Problem"](./images/lecture14/img15.JPG)

!["Problem"](./images/lecture14/img16.JPG)

!["Problem"](./images/lecture14/img17.JPG)

!["Problem"](./images/lecture14/img18.JPG)

!["Problem"](./images/lecture14/img19.JPG)

So, we don't have a Softmax layer after the connected, because here our goal is to directly predict our Q-value functions. 

Yes, so it's more doing regression to our Q-values. 

!["Problem"](./images/lecture14/img20.JPG)

!["Problem"](./images/lecture14/img21.JPG)

!["Problem"](./images/lecture14/img22.JPG)

!["Problem"](./images/lecture14/img23.JPG)

!["Problem"](./images/lecture14/img24.JPG)

!["Problem"](./images/lecture14/img25.JPG)

!["Problem"](./images/lecture14/img26.JPG)

!["Problem"](./images/lecture14/img27.JPG)

!["Problem"](./images/lecture14/img28.JPG)

!["Problem"](./images/lecture14/img29.JPG)

!["Problem"](./images/lecture14/img30.JPG)

!["Problem"](./images/lecture14/img31.JPG)

!["Problem"](./images/lecture14/img32.JPG)

!["Problem"](./images/lecture14/img33.JPG)

!["Problem"](./images/lecture14/img34.JPG)

So, the gradient of an expectation is problematic when p is dependent on theta here, because here we want to take this gradient of p of tau, given theta, but this is going to be, we want to take this integral over tau. 

we can see that, what this will actually look like, right, because now we have a gradient of log p times our probabilities of all of these trajectories and then taking this integral here, over tau. This is now going to be an expectation over our trajectories tau, and so what we've done here is that we've taken a gradient of an expectation and we've transformed it into an expectation of gradients. Right, and so now we can use sample trajectories that we can get in order to estimate our gradient. And so we do this using Monte Carlo sampling, and this is one of the core ideas of reinforce. 

!["Problem"](./images/lecture14/img35.JPG)

Alright, so we have that p of tau is going to be the probability of a trajectory. It's going to be the product of all of our transition probabilities of the next state that we get, given our current state and action as well as our probability of the actions that we've taken under our policy pi. Right, so we're going to multiply all of these together, and get our probability of our trajectory. So this log of p of tau that we want to compute is going to be we just take this log and this will separate this out into a sum of pushing the logs inside. And then here, when we differentiate this, we can see we want to differentiate with respect to theta, but this first term that we have here, log p of the state transition probabilities there's no theta term here, and so the only place where we have theta is the second term that we have, of log of pi sub theta, of our action, given our state, and so this is the only term that we keep in our gradient estimate, and so we can see here that this doesn't depend on the transition probabilities, right, so we actually don't need to know our transition probabilities in order to computer our gradient estimate. And then, so, therefore when we're sampling these, for any given trajectory tau, we can estimate J of theta using this gradient estimate. This is here shown for a single trajectory from what we had earlier, and then we can also sample over multiple trajectories to get the expectation. 

!["Problem"](./images/lecture14/img36.JPG)

Right and so we can see that's what's happening here, where we have pi of a, given s. This is the likelihood of the actions that we've taken and then we're going to scale this, we're going to take the gradient and the gradient is going to tell us how much should we change the parameters in order to increase our likelihood of our action, a, right? And then we're going to take this and scale it by how much reward we actually got from it, so how good were these actions, in reality. 

!["Problem"](./images/lecture14/img37.JPG)

first idea  And so, this is basically saying that how good an action is, is only specified by how much future reward it generates. 

our discount factor's going to tell us how much we care about just the rewards that are coming up soon, versus rewards that came much later on. 

Right, so we were going to now say how good or bad an action is, looking more at the local neighborhood of action set it generates in the immediate near future and down weighting the the ones that come later on. 

!["Problem"](./images/lecture14/img38.JPG)

!["Problem"](./images/lecture14/img39.JPG)

!["Problem"](./images/lecture14/img40.JPG)

!["Problem"](./images/lecture14/img41.JPG)

!["Problem"](./images/lecture14/img42.JPG)

!["Problem"](./images/lecture14/img43.JPG)

!["Problem"](./images/lecture14/img44.JPG)

!["Problem"](./images/lecture14/img45.JPG)

!["Problem"](./images/lecture14/img46.JPG)

!["Problem"](./images/lecture14/img47.JPG)