!["Problem"](./images/lecture5/img1.JPG)

!["Problem"](./images/lecture5/img2.JPG)

!["Problem"](./images/lecture5/img3.JPG)

!["Problem"](./images/lecture5/img4.JPG)

!["Problem"](./images/lecture5/img5.JPG)

!["Problem"](./images/lecture5/img6.JPG)

!["Problem"](./images/lecture5/img7.JPG)

!["Problem"](./images/lecture5/img8.JPG)

!["Problem"](./images/lecture5/img9.JPG)

!["Problem"](./images/lecture5/img10.JPG)

!["Problem"](./images/lecture5/img11.JPG)

!["Problem"](./images/lecture5/img12.JPG)

!["Problem"](./images/lecture5/img13.JPG)

!["Problem"](./images/lecture5/img14.JPG)

!["Problem"](./images/lecture5/img15.JPG)

!["Problem"](./images/lecture5/img16.JPG)

!["Problem"](./images/lecture5/img17.JPG)

!["Problem"](./images/lecture5/img18.JPG)

!["Problem"](./images/lecture5/img19.JPG)

!["Problem"](./images/lecture5/img20.JPG)

!["Problem"](./images/lecture5/img21.JPG)

!["Problem"](./images/lecture5/img22.JPG)

Difference between convolutional layer and this fully connected neural network is that in the convolutional case we want to preserve the spacial structure , instead of strecting it out into a one dimensional layer.

!["Problem"](./images/lecture5/img23.JPG)

!["Problem"](./images/lecture5/img24.JPG)

dot product gives a scalar value whereas cross product gives a vector

we strecth out both vector and multiple i.e.., flatten out the matrixes

We slide the 5*5*3 filter to each spatial space and take the dot product in the convolutional layer.

!["Problem"](./images/lecture5/img25.JPG)

!["Problem"](./images/lecture5/img26.JPG)

!["Problem"](./images/lecture5/img27.JPG)

!["Problem"](./images/lecture5/img28.JPG)

!["Problem"](./images/lecture5/img29.JPG)

!["Problem"](./images/lecture5/img30.JPG)

!["Problem"](./images/lecture5/img31.JPG)

!["Problem"](./images/lecture5/img32.JPG)

Convolutional Layer in working

!["Problem"](./images/lecture5/img33.JPG)

!["Problem"](./images/lecture5/img34.JPG)

!["Problem"](./images/lecture5/img35.JPG)

!["Problem"](./images/lecture5/img36.JPG)

Design Choice : Changing the stride

!["Problem"](./images/lecture5/img37.JPG)

!["Problem"](./images/lecture5/img38.JPG)

!["Problem"](./images/lecture5/img39.JPG)

In practice we don't do strides for which our filter doesn't fit input layer because it may cause assymetric output.

!["Problem"](./images/lecture5/img40.JPG)

Output size is measured by-

!["Problem"](./images/lecture5/img41.JPG)

Don't take the values which gives decimal values.

!["Problem"](./images/lecture5/img42.JPG)

In practice it is common to zero pad-
- all pixles even the corner ones will have a centre position.
- to make the output layer to the size we wanted.

In the above pic we have N=9 F=3 stride=1 
So we have 7*7 output layer after 0 padding!

!["Problem"](./images/lecture5/img43.JPG)

We can 0 pad by any number to contain our stride

!["Problem"](./images/lecture5/img44.JPG)

!["Problem"](./images/lecture5/img45.JPG)

No. of Parameters - 5*5*3 for parameters +1 for the bias term so 76
then there are 10 filters so 76*10.

!["Problem"](./images/lecture5/img46.JPG)

!["Problem"](./images/lecture5/img47.JPG)

!["Problem"](./images/lecture5/img48.JPG)

!["Problem"](./images/lecture5/img49.JPG)

!["Problem"](./images/lecture5/img50.JPG)

What is the intuition behind how we choose stride?
It downsamples and reduces the size of each layer , depends on the resolution.

Diff between conv and neural network is that now conv neuron only looks at a local area.

!["Problem"](./images/lecture5/img51.JPG)

!["Problem"](./images/lecture5/img52.JPG)

!["Problem"](./images/lecture5/img53.JPG)

!["Problem"](./images/lecture5/img54.JPG)

why we need to downsample?
- for fewer parameters
- detailed analysis of the area.

!["Problem"](./images/lecture5/img55.JPG)

!["Problem"](./images/lecture5/img56.JPG)

Usually we don't use zero padding for pooling layer.

!["Problem"](./images/lecture5/img57.JPG)

!["Problem"](./images/lecture5/img58.JPG)

- So the question is, now this looks like just a very small amount of information, right, so how can it know to classify it from here?
    - And so the way that you should think about this is that each of these values inside one of these pool outputs is actually, it's the accumulation of all the processing that you've done throughout this entire network, right. 
    - So it's at the very top of your hierarchy, and so each actually represents kind of a higher level concept. So we saw before, you know, 
    - for example, Hubel and Wiesel  and building up these hierarchical filters  where at the bottom level we're looking for edges, right, or things like very simple structures, like edges. 
    - And so after your convolutional layer the outputs that you see here in this first column is basically how much do specific, for example, edges, fire at different locations in the image. 
    - But then as you go through you're going to get more complex, it's looking for more complex things, right, and so the next convolutional layer is going to fire at how much, you know, 
    - let's say certain kinds of corners show up in the image, right, because it's reasoning. Its input is not the original image, its input is the output, it's already the edge maps, right, so it's reasoning on top of edge maps, and so that allows it to get more complex, detect more complex things. 
    - And so by the time you get all the way up to this last pooling layer, each value is representing how much a relatively complex sort of template is firing. 
    - Right, and so because of that now you can just have a fully connected layer, you're just aggregating all of this information together to get, you know, a score for your class. So each of these values is how much a pretty complicated complex concept is firing.

- So the question is, when do you know you've done enough pooling to do the classification? 
Design choices , try and see

Demo pf Conv

!["Problem"](./images/lecture5/img59.JPG)

!["Problem"](./images/lecture5/img60.JPG)