## Backpropagation and Neural Networks

![](./images/lecture4/img1.JPG)

![](./images/lecture4/img2.JPG)

![](./images/lecture4/img3.JPG)

![](./images/lecture4/img4.JPG)

![](./images/lecture4/img5.JPG)

We need to find df/dx ,  df/dy , df/dz

We start from the back calculating df/df which is equal to 1 . Then move backwards like df/dz , df/dq , df/dx , df/dy

![](./images/lecture4/img6.JPG)

![](./images/lecture4/img7.JPG)

![](./images/lecture4/img8.JPG)

![](./images/lecture4/img9.JPG)

![](./images/lecture4/img10.JPG)

![](./images/lecture4/img11.JPG)

![](./images/lecture4/img12.JPG)

### BackPropagation step by step

![](./images/lecture4/img13.JPG)

![](./images/lecture4/img14.JPG)

![](./images/lecture4/img15.JPG)

![](./images/lecture4/img16.JPG)

![](./images/lecture4/img17.JPG)

![](./images/lecture4/img18.JPG)

![](./images/lecture4/img19.JPG)

We can group together some compututional nodes to a more complex node. See the pic below.

![](./images/lecture4/img20.JPG)

### Backpropagation different gate meanings

![](./images/lecture4/img21.JPG)

![](./images/lecture4/img22.JPG)

So in max we give the previous grad value to the variables grad who have the highest value and other variables we give 0.

In mul/ multiply gate we just multiply the prev grad value to the opposite value of the current variable like for x = grad*y and for y = grad*x.

![](./images/lecture4/img23.JPG)

When gradient are like joining to a node from two different nodes we just add up the gradient coming from the two nodes to the node they a joining into 

Now the grad of node 1 is eual to gradient/grad of node 2 plus gradient of node 3.

![](./images/lecture4/img24.JPG)

![](./images/lecture4/img25.JPG)

Jacobian matrix is particial derivative of each output wrt to each input.

![](./images/lecture4/img26.JPG)

In practice we perform computation on 100s of inputs so jacobian is very large.

![](./images/lecture4/img27.JPG)

Our Jacobian matrix is going to be a diagonal matrix.

![](./images/lecture4/img28.JPG)

![](./images/lecture4/img29.JPG)

![](./images/lecture4/img30.JPG)

![](./images/lecture4/img31.JPG)

Forward we compute computational graph.
Backward we compute gradient.

![](./images/lecture4/img32.JPG)

![](./images/lecture4/img33.JPG) 

![](./images/lecture4/img34.JPG) 

![](./images/lecture4/img35.JPG) 

## Neural Networks

Neural networks are just functions just stacked on layer by layer

![](./images/lecture4/img36.JPG) 

![](./images/lecture4/img37.JPG) 

![](./images/lecture4/img38.JPG)

![](./images/lecture4/img39.JPG)

![](./images/lecture4/img40.JPG)

![](./images/lecture4/img41.JPG)

![](./images/lecture4/img42.JPG)

![](./images/lecture4/img43.JPG)

![](./images/lecture4/img44.JPG)

![](./images/lecture4/img45.JPG)

![](./images/lecture4/img46.JPG)