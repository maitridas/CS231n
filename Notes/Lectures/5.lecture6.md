## Training Neural Network

!["Problem"](./images/lecture6/img1.JPG)

!["Problem"](./images/lecture6/img2.JPG)

!["Problem"](./images/lecture6/img3.JPG)

!["Problem"](./images/lecture6/img4.JPG)

!["Problem"](./images/lecture6/img5.JPG)

### Sigmoid function

!["Problem"](./images/lecture6/img6.JPG)

- What does the first problem means?

!["Problem"](./images/lecture6/img7.JPG)

- x= -10 gradient is 0, kill off the gradient and not going to get gradient flow
- x= 0 gradient is ok
- x= 10 gradient is 0, kill off the gradient and not going to get gradient flow

Second problem with sigmoid func is -

!["Problem"](./images/lecture6/img8.JPG)

!["Problem"](./images/lecture6/img9.JPG)

!["Problem"](./images/lecture6/img10.JPG)

Inefficient gradient updates , w will increase or decrease in the same direction, see the zig zag part we are only allowed to travel the w value in the red path , whereas we could have just travelled along blue line.

That why we need 0 mean so that we have +ve and -ve values and don't run into such problems.

Third problem with sigmoid func is -

!["Problem"](./images/lecture6/img11.JPG)

### tanh

!["Problem"](./images/lecture6/img12.JPG)

Looks similar to sigmoid difference is its zero centered.

### ReLU

!["Problem"](./images/lecture6/img13.JPG)

Does not saturate in positive reason.

!["Problem"](./images/lecture6/img14.JPG)

- x=10 ->linear
- x=0 -> undefined but in practice we say its 0
- x=-10 -> 0  killing the gradient

!["Problem"](./images/lecture6/img15.JPG)

Bad relu reason

- bad initialization
- learning rate too high