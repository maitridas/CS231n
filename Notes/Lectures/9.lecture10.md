# Recurrent Neural Networks

!["Problem"](./images/lecture10/img1.JPG)

!["Problem"](./images/lecture10/img2.JPG)

!["Problem"](./images/lecture10/img3.JPG)

!["Problem"](./images/lecture10/img4.JPG)

!["Problem"](./images/lecture10/img5.JPG)

!["Problem"](./images/lecture10/img6.JPG)

!["Problem"](./images/lecture10/img7.JPG)

!["Problem"](./images/lecture10/img8.JPG)

!["Problem"](./images/lecture10/img9.JPG)

-  So in general, a recurrent neural network is this little, has this little recurrent core cell and it will take some input x, feed that input into the RNN, and that RNN has some internal hidden state, and that internal hidden state will be updated every time that the RNN reads a new input. And that internal hidden state will be then fed back to the model the next time it reads an input. And frequently, we will want our RNN"s to also produce some output at every time step, so we'll have this pattern where it will read an input, update its hidden state, and then produce an output. 

!["Problem"](./images/lecture10/img10.JPG)

- So inside this little green RNN block, we're computing some recurrence relation, with a function f. So this function f will depend on some weights, w. It will accept the previous hidden state, h t - 1, as well as the input at the current state, x t, and this will output the next hidden state, or the updated hidden state, that we call h t. And now, then as we read the next input, this hidden state, this new hidden state, h t, will then just be passed into the same function as we read the next input, x t plus one. And now, if we wanted to produce some output at every time step of this network, we might attach some additional fully connected layers that read in this h t at every time step. And make that decision based on the hidden state at every time step. 

-  we use the same function, f w, and the same weights, w, at every time step of the computation. 

!["Problem"](./images/lecture10/img11.JPG)

- So here, we have this same functional form from the previous slide, where we're taking in our previous hidden state and our current input and we need to produce the next hidden state. And the kind of simplest thing you might imagine is that we have some weight matrix, w x h, that we multiply against the input, x t, as well as another weight matrix, w h h, that we multiply against the previous hidden state. So we make these two multiplications against our two states, add them together, and squash them through a tanh, so we get some kind of non linearity in the system. 