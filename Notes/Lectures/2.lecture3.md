![](./images/lecture3/img1.JPG)

![](./images/lecture3/img2.JPG)

```
f(x,W) = Wx
```

![](./images/lecture3/img3.JPG)

Multiclass SVM is for multiple class there us something like binary class svm which is only for 2 class.

![](./images/lecture3/img4.JPG)

Here 1 is a safety margin it can be any value we generally take 1 here safety margin is set so that we can show that value of the true category is much much larger than the false category. 1 ia arbitary. We only care about the relative scores.

![](./images/lecture3/img5.JPG)

this style of taking loss were we always take max of 0 or something is called hinge loss.This name comes from the shape of the graph 

x axis is syi and y axis loss

Loss is saying that we are happy that true score is much higher than the other class scores.

![](./images/lecture3/img6.JPG)

![](./images/lecture3/img7.JPG)

- Min loss - 0
- Max loss - infinity

![](./images/lecture3/img8.JPG)

The answer to the question is number of (class - 1). This is a debugging strategy very useful.

![](./images/lecture3/img9.JPG)

Answer is loss increases by one . The answer won't change but its just by convention we do this.

If we use mean instead of sum nothing will change.

![](./images/lecture3/img10.JPG)

The problem will change here. Here the idea is are changing the trade of between good and bad in a non linear way so this would compute a different linear function. 
Squared loss says that we really hate loss.
Depends on application.

![](./images/lecture3/img11.JPG)

A trick here is to notice that we are putting 0 in the margin of the correct class. 

![](./images/lecture3/img12.JPG)

![](./images/lecture3/img13.JPG)

![](./images/lecture3/img14.JPG)

Regularization restricts the complexity of the model.How?

![](./images/lecture3/img15.JPG)